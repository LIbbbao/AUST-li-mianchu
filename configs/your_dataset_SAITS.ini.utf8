[file_path]  
; prefix of saving dir，用于保存路径的前缀   
prefix = .  
; base dir, in which the dataset is saved，数据集的根目录   
dataset_base_dir = generated_datasets  
; 根目录，用于保存结果   
result_saving_base_dir = results  
; dir to save models，用于保存模型的目录   
model_saving_dir = ${prefix}/${result_saving_base_dir}/${model:model_name}/models   
; dir to save graphs, which will be plotted in model testing，用于保存测试时生成的图形   
test_results_saving_base_dir = ${prefix}/${result_saving_base_dir}  

[dataset]  
; dir name of your dataset, dataset_base_dir/dataset_name should be the absolute path of dataset，数据集名称，路径应为 dataset_base_dir/dataset_name   
dataset_name = your_dataset  
; sequence length, namely the num of past days，序列长度，即过去的天数   
seq_len = 11  
; num of input features，输入特征的数量   
feature_num = 13  
; batch size，批次大小   
batch_size = 128  
; num of workers in dataloader，数据加载器中的工作线程数   
num_workers = 4  
; evaluate every n steps，每 n 步进行一次评估   
eval_every_n_steps = 60  

[model]  
; name of your model, will be the name of dir to save your models and logs，模型名称，将成为保存模型和日志的目录名称   
model_name = AUST_gait_prediction  
; whether concat input with missing mask，是否将输入与缺少的掩码连接   
input_with_mask = True  
; model type, Transformer/SAITS，模型类型，可以是 Transformer 或者 SAITS   
model_type = Transformer  
; num of layer groups，层组的数量   
n_groups = 2  
; num of group-inner layers，组内层数   
n_group_inner_layers = 1  
; how to share parameters, inner_group/between_group，参数共享策略，inner_group 或 between_group   
param_sharing_strategy = inner_group  
; model hidden dim，模型的隐藏层维度   
d_model = 256  
; hidden size of feed forward layer，前馈层的隐藏尺寸   
d_inner = 128  
; head num of self-attention，自注意力头的数量   
n_head = 4  
; key dim，键的维度   
d_k = 64  
; value dim，值的维度   
d_v = 64  
; drop out rate，dropout比率   
dropout = 0.1  
; whether to apply diagonal attention mask，是否应用对角注意力掩码   
diagonal_attention_mask = False   

[training]  
; whether to have Masked Imputation Task (MIT) in training，在训练中是否进行掩码填补任务（MIT）  
MIT = True  
; whether to have Observed Reconstruction Task (ORT) in training，在训练中是否进行观测重建任务（ORT）  
ORT = True  
; max num of training epochs，最大训练周期数   
epochs = 100  
; which device for training, cpu/cuda，训练设备，cpu或者cuda   
device = cuda  
; learning rate，学习率   
lr = 0.000669204734411692  
; weight for reconstruction loss，重建损失的权重   
reconstruction_loss_weight = 1  
; weight for imputation loss，填补损失的权重   
imputation_loss_weight = 1  
; patience of early stopping, -1 means not applied (current early stopping is based on total loss)，早停的耐心值，-1表示不应用（当前早停基于总损失）   
early_stop_patience = 30  
; what type of optimizer to use, adam/adamw，使用哪种类型的优化器，adam或者adamw   
optimizer_type = adam  
; weight decay used in optimizer，优化器中的权重衰减   
weight_decay = 0  
; max_norm for gradient clipping, set 0 to disable，梯度裁剪的最大范数，设置为0表示禁用   
max_norm = 0  
; strategy on model saving, all/best/none. If set as none, then do not save models (mode for hyper-parameter searching), 模型保存策略，all/best/none。如果设置为none，则不保存模型（用于超参数搜索模式）   
model_saving_strategy = best  

[test]  
; whether to save imputed data，是否保存填补的数据   
save_imputations = True  
; name of model your select for testing，选择的测试模型的名称   
step_184 = model_trainStep_11040_valStep_184_imputationMAE_0.1893  
; absolute path to locate model you select，选择的模型的绝对路径   
model_path = ${file_path:model_saving_dir}/${step_184}  
; path of dir to save generated figs (PR-curve etc.)，保存生成图形（PR曲线等）的目录路径   
result_saving_path = ${file_path:test_results_saving_base_dir}/${model:model_name}/step_18